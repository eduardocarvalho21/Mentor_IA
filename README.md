# MentorIA - Local RAG System with Strict Validation

O **MentorIA** √© um assistente inteligente baseado em arquitetura **RAG (Retrieval-Augmented Generation)** que roda 100% localmente. O projeto foi desenhado para analisar documentos t√©cnicos e responder perguntas com **zero custo de API**, privacidade total e, principalmente, **mecanismos robustos contra alucina√ß√£o**.

---

## üöÄ Diferenciais de Engenharia

Diferente de wrappers simples de IA, o MentorIA implementa l√≥gicas avan√ßadas de backend:

- **üõ°Ô∏è Guardrails Anti-Alucina√ß√£o:** Utiliza um algoritmo de *Cosine Similarity* com threshold rigoroso (`0.50`). Se a pergunta foge do contexto do documento (ex: perguntar de futebol em um texto m√©dico), o sistema bloqueia a resposta antes mesmo de chamar a LLM.
- **‚ùÑÔ∏è Tratamento de Cold Start:** Implementa√ß√£o de um padr√£o de **Retry com Backoff Exponencial**. Se o modelo local (Ollama) estiver descarregado da RAM, o backend aguarda e retenta a conex√£o automaticamente, evitando erros para o usu√°rio.
- **‚ö° Busca Vetorial Otimizada:** Utiliza √≠ndices **HNSW** (Hierarchical Navigable Small World) no PostgreSQL, permitindo buscas sem√¢nticas em milissegundos.
- **üí¨ Streaming em Tempo Real:** Respostas geradas token a token para uma UX fluida.

---

## üõ†Ô∏è Tech Stack

### Core
- **Frontend:** [Next.js 14](https://nextjs.org/) (App Router), Tailwind CSS.
- **Backend:** Next.js Server Actions & API Routes.
- **Linguagem:** TypeScript.

### Dados & IA
- **Database:** [Supabase](https://supabase.com/) (PostgreSQL + `pgvector`).
- **ORM:** [Drizzle ORM](https://orm.drizzle.team/).
- **AI Engine (Local):** [Ollama](https://ollama.com/).
- **Modelos:**
  - LLM: `llama3.2:1b` (Gera√ß√£o de texto r√°pida e leve).
  - Embeddings: `nomic-embed-text` (Vetoriza√ß√£o de alta fidelidade).

### Ingest√£o
- **Scripts:** Python (para processamento de PDFs e *chunking*).

---

# Baixe os modelos necess√°rios

```bash
ollama pull llama3.2:1b
ollama pull nomic-embed-text

```bash
-- Habilitar pgvector
create extension vector;

-- Tabela de Embeddings (exemplo simplificado)
create table embeddings (
  id serial primary key,
  content text not null,
  embedding vector(768) -- Ajuste conforme o modelo nomic
);

--  √çndice HNSW para performance extrema
create index on embeddings using hnsw (embedding vector_cosine_ops);

```bash
git clone https://github.com/eduardocarvalho21/Mentor_IA.git
cd mentoria
npm install

```bash
DATABASE_URL=postgres://user:pass@host:5432/db
NEXT_PUBLIC_API_URL=http://localhost:3000

```bash
npm run dev

Acesse **http://localhost:3000.**

Como Funciona o "C√©rebro" (Fluxo RAG)
Input: O usu√°rio faz uma pergunta.

Embedding: O backend converte a pergunta em vetor usando nomic-embed-text.

Vector Search: O Supabase busca os trechos de texto mais similares.

Filtro (The Guardrail):

Se similaridade < 0.50 ‚û°Ô∏è Retorna "N√£o consta no documento."

Se similaridade >= 0.50 ‚û°Ô∏è Segue para o pr√≥ximo passo.

Gera√ß√£o: O contexto recuperado + a pergunta s√£o enviados ao Llama 3.2 com um System Prompt estrito.

Output: A resposta √© transmitida via stream para o frontend.

ü§ù Contribui√ß√£o
Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para abrir Issues ou Pull Requests.

üìù Licen√ßa
Este projeto est√° sob a licen√ßa MIT.

<div align="center"> Desenvolvido por <strong>Eduardo Pereira de Carvalho</strong>


<span>Software Developer | Fullstack | AI Enthusiast</span> </div>
